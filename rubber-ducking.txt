I am sure I am going to regret some of this file structure lol 

ok so I can use some sample images to detect lying and other emotional intelligence questions based on pictures 
	I can probably get them by looking back as the Unanimous Paper on EQ (https://unanimous.ai/wp-content/uploads/2018/09/Amplifying-Social-Intelligence-IEEE-ai4i-2018.pdf)

let's try to update the rank of the questions based on their votes 
	this is the way you sort an array of objects by value (https://www.javascripttutorial.net/array/javascript-sort-an-array-of-objects/)
		I also checked that this works in the browser
	now I just have to do it in state 

now that I've hooked up Firebase (get it, "hooked" up ahahha), let's figure out the optimal JSON structure
	uhm, I guess I have not figured out how to update Firebase from the app lol 
	how do I update the rank of the questions
		I should probably just sort them by value
		wait, it's already there in the code! 
	maybe I should calculate the score by adding the scores from all participants
		Yea, that makes sense  

	firebase {
		combo {
			pictureA: "dogA_id",
			pictureB: "dogB_id",
			features: {
				"preset_feature_A": {
					"score": {
						0: 0
						1: .2
						2: .3
					}
					"weight": 12
				},
				"generated_feature_A": {
					"score": {
						0: 0
						1: .5
						2: .2
					}
					"weight": 11
				},
			}
		}
	}

let's see, how do I best leverage the onChange on the range input
	I should probably do this when I move everything to the new view
	Yea, bc new view --> table with the input 
	I should probably implement this rn and then just move the logic to the new view when I get to it
	hook + push the object on submit vs. update Firebase continously 
		The issue with updating Firebase right away is that I'd have to somehow track who voted for what 
			I guess I could use the push.key trick and save that into state and just update that key all the time
			{ "user_id": vote } vs { "key": {"user_id": vote } }


let's create the round robin system 
	the pic pair is going to be in the JSON already 
		I'm going to generate the base JSON with a python script
	
Ok the key now is to do a limited view change
	Thinking of holding a title in the middle and then rendering the other components invisible
	I wonder how the original Flock paper did this
		"Flock asks crowd members to guesswhich of two examples is from a positive class and which isfrom the negative class, then write a reason why. These rea-sons become features: Flock automatically clusters the rea-sons, then recruits crowds to normalize each cluster and pro-duce features. These features are then used by the crowd toannotate each example."
		"Flock trains multiple models with different fea-ture subsets. It does so to show the user the performance of multiple prediction methods on the test set. These models in-clude Crowd prediction(a baseline asking workers directlyto guess the correct label for the example), ML with off-the-shelf(the chosen machine learning model using only out-of-the-box features such as n-grams), ML with crowd(the cho-sen machine learning model using only crowd features), or Hybrid(the full Flock model using both machine and crowdfeatures)."
		"Previous research on analogical encoding has found thatwhen considering single examples, people tend to focus onsurface-level details, but when they compare examples, theyfocus on deeper structural characteristics"
		"Flock launches 100c omparison tasks with three workers each per dataset, result-ing in 300 nominated features for about six cents per label ($20 total)."
	Ah ok, so they have one task to generate features, and one task to vote on them
		They are two separate tasks
		"The features have considerable overlap, and manual cat-egorization suggested that the 300 nominations typically clus-tered into roughly 50 features."
		"First, the system splits each responseinto multiple suggestions (e.g. by sentence, newlines, andconjunctions such as “and”). The system then performsk-means clustering (k= 50) using tf-idf weighted bigram text4 features. With these clusters in hand, Flock launches anothertask to CrowdFlower showing workers one cluster at a timeand asking them to summarize each into a single represen-tative question (or feature) that has a yes or no answer (e.g.,“Is this person shifting their eyes a lot?”). Three candidatefeatures are generated for each cluster."

MapReduced version
	Feature Generation 
		Should we do 2 different dogs and 1 that is similar to at least one of them? 
		Similarity + Difference
			Ask about similarities 
			Ask about differences 
		Cluster
			Basic n-gram stuff 
		Extract top 3 summary features 
	Feature Evaluation
		Rank is individual 
		Scoring is continous 

ok so let's figure out the sequence to pull of the rewrite
	I need to probably make separate pages, so that each link links to a different task 
	the features generated don't need to be linked by users 
		they can be just a part of JSON, a separte child
	then from that child I build a page where it shows the n-gram and the user submist the summary 
		the summary features need to be binary 
		then voting happens, which is how the top 3 gets selected
			which means I have to save this per user, or at least find a way to maintain the voting
				I can just not show current votes to the user and have them vote without registering who they are
		the summaries are saved on a new child 
	after clustering, I move on to the feature evalution 
		both rank and scoring need to be attached to an individual user 

ok so let's structure this DB
	firebase {
		suggestions: {
			idA: "suggestion from user A",
			idB: "suggestion from user B"
		},
		clusters: {
			clusterA: {
				suggestions: [ngramA, ngramB],
				summaries: {
					summary_id_1 {
						id_a: "summarizing cluster A",
						vote: 0
					},
					summary_id_2 {
						id_a: "summarizing cluster A",
						vote: 0
					}
				}

			}
		},
		combo {
			pictureA: "dogA_id",
			pictureB: "dogB_id",
			features: {
				"summary_id_1": {
					"score": {
						0: 0
						1: .2
						2: .3
					}
					"weight": {
						0: 0
						1: .2
						2: .3
					}
				},
				"summary_id_2": {
					"score": {
						0: 0
						1: .5
						2: .2
					}
					"weight": {
						0: 0
						1: .2
						2: .3
					}
				},
			}
		}
	}

ok so I'm at the suggestions page
	I already implemented the form to submit the suggestion, now I just have to do voting
	I can probably re-implement the logic from the v1
	The voting here though is not attached to the user, it's more like a usual upvote/downvote mechanism
		This means I need to add a "votes" value-key pair to each suggestion child
	I keep thinking that I should have separate phases for submitting and voting
		I am so dumb lol
		I already implemented this!!
	Why is it not updating?
		bc I was sorting the snapshots as opposed to the actual object
	Now I just need to limit the voting to just once per item
		I can disable the button for that vote
		how do I do this without having to set up a hook or a global property
		boom!

now I let's do the eval page
	mostly it's the same as the v1, just removing the option
	ok so I probalby don't need to change that much logic, bc I can set up the features to be evaluted based on teh results from the summary task 
		I'll automate this some day 
	ok so assume you already have the right features for each pair of dog pictures, what do you need to change? 
		both rank and scoring need to be attached to each user
	do I want to structure the data to be feature centric by having user-identified updates to the children of the feature? 
		Or do I want to make it user centric? 
		well, at a basic level it'd make it a lot easier to have this be pair centric to build the graph afterwards 
	uhm, so when the user submits, they are submitting a score and a rank 
		obviously score comes first, replicating the staging process that we have in the summary task 
		so all I need to do is restructure "weight" to be like "score" and similarly attach the value to the user-id
			the order somewhat doesn't matter as long as I have the id
	gosh, wouldn't be great if this was a carosel type thing
		#scope-creep
	ok so now even the next function operates properly

ok so let's see, where am I now? 
	I have to create a closing tastement for every task
		That's not too bad, especially bc I need to start with task 1

ok we are back
	so, I already implemented the two phases of Task 2, so at this point I just have to just restructure it so that it's based on clusters
	I need to update the structure of the Firebase JSON, and then ensure the updates are flowing smoothly 
		cluster1 {
			suggestions {
				0: "lorem ipsum"
			}
			summaries {
				summary_A {
					userID: userID
					text: "lorem ipsum"
					votes: 0
				}
				summary_B {
					userID: userID
					text: "lorem ipsum"
					votes: 0
				}
			}
		}
	should voting the top 3 features happend after every cluster?
		probably
	let's start by checking if I can move across clusters easily
		woot woot!
	now let's see if I can uplaod the summaries correctly
		uploading works, but now I have to do the voting correctly
		ok so now I have to update the votes correctly
	nice, so now let's go to the top items from the cluster

Duplicate checker
	it should be pretty easy
	I need to loop through all the Firebase answers so far
		let's start with this
	I get all the answers and remove filler words like "the", "are", "dogs", and "both"
	then I run the same function on the submitted answer and check in whether any key terms left are already in the list
	this shouldn't be too hard, but I might have to read up on some NLP shit
	ok so this works, but maybe reduce the answers into a single large array to check for duplicates is not the right appraoch
		after all, you shouldn't be penalized for using the word "long" if it's referring to a different body part
		so then I should make a regular loop that just terminates if too many words are included
	wow, so update the hook doesn't work within this while loop for some reason lol
		gotta implement some crappy solution
	I want to show them the prior answers, which shouldn't be hard
		I worry I'll upset them though if they try for like 30m and don't come up with anything new
		I could show them different dogs after like 5 attempts
			yea, and then reset after 5

clustering
	first, I need to re-run task 1
		let's download all the prior responses
		then I'll narrow it to the last 50 bc they came from the 10 Master Turkers
		and then I'll do a clean slate
	why isn't it updating on vercel?
		ok now it works
		so strange lol
	ok back to clustering
		now I'm going to take the good suggestions and run the n-gram script
	what a win for real time patching ahah
	ok so back to clustering now
		this mofo ain't working properly lol
		I mean, the DB scan is working right, but some of these features include the verbs
		I need to extrac key words maybe?
			that's what the stopwords nltk error is about!!
				great way of debugging through permissions!
		ok this is starting to make more sense

ok let's do this: Task 3
	first, let's figure out the features from the clusters
		let's first run an experiment with the new suggestions
			same fur color
			same size
			patches? 
			droopy face
			length of neck
			thin mid-section
			relaxed ears
			floppy ears
			looseness of fur
			stomach curvature
			length tails
			length of jaw
			ferociousness 
			curl of tail
			skin around nose
			teeth
		yes, let's try these
			let's generate them with a script
	also, rn it's 5 dogs but it ends up being 20 comparisons
		should I do more?
			Nah, not yet
	ok so we got the features, but now let's add some anti-trolling
		I could make them answer each question separately
			that way the next button only shows up at the end
		ok, fixed the default value so it still moves
		I could do question by question, with some duplicates as the attention check
			Yea, and the whole thing has to start over if they fail at answering in the same exact way
			This is easier said than done ahah
			also, the Next button should only show up at the end
			maybe I should use UseEffect to initialize the list of features and then have a hook that just loops through them
			oh yea, wait it's that easy bc I can just figure out the keys each time
				nice. Not super-clean but not dirty either
			now I have to create the right event for the NextFeature button to show up
				I really don't want to have to create a separate hook for this
				boom!
			nice, so now everything is place to implement the attention check
				ok let's do this
				how should I structure this? 
					I could initialize and select some random int within the number of features where I inject a repeated question
						then I check double check the values
							I feel like this is not the hard part
					yea, I could select some random int
						actually, select a random feature and then a random time to ask it again
						and then through conditinal checking I inject it again!!
							let's try it
						so close: you've got this
				ok so now you are accessing firebase effectively
			this should be a simple comparison
				oh shit, what if I get the same issue about internal state updates like I did last time...
					wait jk it just worked!
				now the issue is enabling the person to keep going
					should I make a new hook where the status is passed?
				nice! now it works
					let's just add attention checks to the script

ok, so first let's fix this bug
	for some reason, past pair0 it's only showing them the rank features as opposed to scoring
		was this for all of them?
			yes
	I wonder if it's bc I'm not switching stages properly
		let's check the logic when the dogPair switches
		weird...it just worked
			might just have to re-run it lol

ok so let's implement the progress bar
	go React Boostrap, so much easier lol

time to do the graph
	let's get this d3 thing going on
	let's actually take the time to learn this

let's figure out this PCA ranking
	ok so now that I talked to Daniel this makes more sense
		breed_name is the last value in the array
		multiple samples for each breed
	first, ima average the multiple samples
		heck yes
	then ima go through each pair and compute the Eucledian distance
		now the key here is to run through all the pairs
	then ima rank the pairs by Eucledian distances and select the top 50
	 	top 50 means 2450 (50 * 49) pair-wise comparisions lol
	 		this is a lot of comparisions, might have to trim it to just 5
	 	actually, it's just 50 pairs, not 50 dogs so we are good lol

ok so first let's re-run Task 1
	how should I do this? 
	I have the top 50 pairs, so I just have to break that down into the actual dogs
	I can likely update the base_json script and generate the right sequence that way
	actually, this is probably easier bc I can just append this to the script
	wait, I don't like shifting dogs every pic
		let's make it 3 suggestions per dog pair

let's do this graph
	let's implement the d3 version first
	good page (https://gist.github.com/alexcjohnson/a4b714eee8afd2123ee00cb5b3278a5f)
	good examples (https://medium.com/@Elijah_Meeks/interactive-applications-with-react-d3-f76f7b3ebc71)
	done!!!

	now let's break down what the JSON looks like to load it
		{
			"nodes": [
				{
				"id": "Myriel",
				"group": 1
				},
				{
				"id": "Napoleon",
				"group": 1
				},
			],
			"links": [
				{
				"source": "Napoleon",
				"target": "Myriel",
				"value": 1
				},
				{
				"source": "Mlle.Baptistine",
				"target": "Myriel",
				"value": 8
				}
			]
		}

	let's figure this out for just one pair

	wait, actually I should probably start with links and then construct the nodes from the links
		#BigBrain
		wait a minute, I'm not actually comparing individual dogs against each other
			I'm comparing two dogs: that's the data I have

let's extract the new features
	I should have the script already, so I just need to download the JSON from Firebase and re-run it #fingersCrossed
	this worked!!!

elo-based redesign
	first, pull up the code for CrowdElo
	focus!! the first pomodoro is to tame the monkey mind! 
	what does the DB model need to look like? 
		breed_name
			picture: "lol.jpg"
			features
				feature_name
					weight: numOfTimesTheComparisonTookPlace
					score: 1500



	most of the changes will be to the frontend
		gotta figure out how to do an attention check 

	maybe I should track which pairwise comparisions have been done
		that's where the PCA values come in 

	let's start with redesigning the model
		nice

	let's not worry about optimizing the pair-wise or feature-wise comparision quite yet

	let's actually take a look at the graph

let's get started
	I should look at the graph as well as some point today, but let's focus on the attention check
		jk, let's do the graph real quick, now that I have some data

	last time, I implemented the attention check by picking a random question and registering the response to ensure it matched when asked again
		I can do a similar thing by picking a prior comparision the person and ask it again
		they key is to register scores based on userID
	I also should have some grounding results, like the darkness of the skin color

	oh this reminds me, I need to add progress bars
		yea, let's do this real quick

	ok back to the attention check
		how did I implement last time? 
		at a basic level, I need to pick a random feature and just ask it again

		"attention_checks" {
			userID: {
				dogID: 3
				dogID: 4
				answer: 0
			}
		}

ok let's go 
	the Sopranos is such a good show for this
	let's manipulate feature inclusion

ok let's do this attention check
	nevermind, let's fix this bug
		jk I was looking at the wrong page ahah
	I'm def going to have to restructure the schema a little bit
		already I was thinking of saving the participant id in the score update so I can remove them more easily
	I should keep track of the pairwise comparision matrix in a separate branch
	I also need do a bootstrap task
	yea, I should pivot away from doing the attention check today

	should I develop a tiny server, maybe using Flask, and deploy on Heroku to track the pairwise matrix
		according to this I'd still have to unwrap it into pairs (https://stackoverflow.com/questions/56970259/how-to-model-a-matrix-table-in-django)

	the pairwise comparision matrix is the first step
		it'd be super easy to add the PCA results 
		0 {
			1 {
				wins: {
					userA: 300
				}
				losses: {
					userB: 200
					userC: 120
				}
				pca: 14.5
			}
		}
		I really don't want to duplicate this data though

	for the bootstrap task, I just have to modify the evaluation page
		yea, and just have 1 user put in the initial value unless they fail the check 

	let's fully flesh out the pair selection algo 
		First, Ima use the boostrap values to generate some level of transitivity
			it needs it's own branch in the DB
		I need check on the PCA and prioritize pairs where the distance is low
		the key is to figure out the relationship between the PCA distance, the distance from boostrapping, and the elo update
			yea, bc on some level I'm trying to falsify a large phenotype-genomic ratio
		wouldn't it make sense to take the winner and make it compete against the next best in line until it drops? 
			but I need to keep track of dominated pairs, especially for upsets 
		should I loop through all the features for each pair? 
		the algo runs client-side by breaking down the matrix
		they key to PAPRIKA is that as long as the hierachy makes sense recursively, then I can trust the overall ranking
			which simplifies the problem away from having to do all pairwise comparisions
			yea so instead of random I should keep the winner and iterate until it loses
			do I stop once I reach a duplicate?
			

	just had a realization about the attention check: if there is a duplicate, just check if the answers have been the same

time to finally do this horse scraper
	first, let's get the links
		https://www.horsebreedspictures.com/gypsy-horse.asp
			it's nice that it automatically redirects to the right horse
		http://www.theequinest.com/breeds/arenberg-nordkirchener/
		http://www.fao.org/dad-is/browse-by-country-and-species/en/
	next, let's get teh list of horse breeds
	ok let's draft out a plan for this
		read the file to make a list of breeds
		enter them on each website
			each website should have its own scraping function
				thus I feed the breed name as the argument
		I need to put into a matrix

first let's do the bootstrap 
	I need to resurrect the old version and make push everything to the Firebase
	let's adapt it to the new thing
	now let's redraw the default graph to just include height and weight

ok let's do this ground truth ranking comparision 
	first I need to get height and weight data 
		jeez, I can't believe it took 3 pomodoros to do this lol
	next I need to download and cleanup the flock data
		this was easier than I expected
	then I need to compute the Average Reciprocal Hit Ranking
		this library is perfect http://ocelma.net/software/python-recsys/build/html/evaluation.html
			doesn't work with python3 lol
		let's compute it ourselves (https://en.wikipedia.org/wiki/Mean_reciprocal_rank)
	seems like the Spearman is the better option 

ok let's update the pair selection algo 
	first we are going to have a winner take down system 
		this was easier than expected
			shooutut to maintanble code 
	second the attention check 
		first, I have to track all of the new score updates so I can remove them
		next I need to track the history 
		I then need to periodically pick a random comparision so I can run it again 
			see, but then what happens if someone has already run a comparision...
				I can can run a complex backtrack
					which wouldn't work bc I can't do it simultaneously to all of them
		I need to just track wins and losses 
			this is good for the rest of the system I guess
			userID {
				win/loss: -1 or 1
				opponent: opponentID
			}
			the thing is, I need to know when these happened
			should I be tracking it per user? 
				that would easily track the history 
				yea, that would make it super easy to remove the prior answers 
			no, I need to track it as a global history otherwise I lose the sequence 
				userID {
					winner: dogC
					loser: dogD
				}
				userID {
					winner: dogA
					loser: dogB
				}
				OMG and this is super easy to then do the attention check 
			I can do both

let's re-run this now
	the progress bar is not resetting
		fixed
	ok now I added the end
		let's restructure this system so that the reward only happens at the end
	now let's limit to the 50 dogs 
		first, I need to generate a file with the overlapping names
		I guess it's 100 lol
	good to add the message about the troll-check

TO-DO sidenote
	might be good to have a survey at the end
	let's not forget about the crypto mining in the background






